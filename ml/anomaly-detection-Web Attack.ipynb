{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3bd2vXMeSx6s"
   },
   "source": [
    "# Steps for implementation\n",
    "\n",
    "Reading CSV\n",
    "\n",
    "Data Cleaning\n",
    "\n",
    "Features\n",
    "\n",
    "Applying HBOS to detect outliers from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O46P3metTaPc"
   },
   "outputs": [],
   "source": [
    "#Importing Necessary Modules\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split     # import module for train test split\n",
    "from pyod.models.hbos import HBOS\n",
    "from pyod.utils.data import evaluate_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprosess(df):\n",
    "\n",
    "    #remove rows with any values that are not finite\n",
    "    df = df[np.isfinite(df).all(1)]\n",
    "    \n",
    "    df=df.dropna( axis=0, how='any')\n",
    "    df=df.replace(',,', np.nan, inplace=False)\n",
    "    df=df.drop(columns=[' Fwd Header Length.1'], axis=1, inplace=False)\n",
    "\n",
    "    df.replace(\"Infinity\", 0, inplace=True)\n",
    "    df['Flow Bytes/s'].replace(\"Infinity\", 0,inplace=True)\n",
    "    df[\" Flow Packets/s\"].replace(\"Infinity\", 0, inplace=True)\n",
    "    df[\" Flow Packets/s\"].replace(np.nan, 0, inplace=True)\n",
    "    df['Flow Bytes/s'].replace(np.nan, 0,inplace=True)\n",
    "    df[\"Bwd Avg Bulk Rate\"].replace(\"Infinity\", 0, inplace=True)\n",
    "    df[\"Bwd Avg Bulk Rate\"].replace(\",,\", 0, inplace=True)\n",
    "    df[\"Bwd Avg Bulk Rate\"].replace(np.nan, 0, inplace=True)\n",
    "    df[\" Bwd Avg Packets/Bulk\"].replace(\"Infinity\", 0, inplace=True)\n",
    "    df[\" Bwd Avg Packets/Bulk\"].replace(\",,\", 0, inplace=True)\n",
    "    df[\" Bwd Avg Packets/Bulk\"].replace(np.nan, 0, inplace=True)\n",
    "    df[\" Bwd Avg Bytes/Bulk\"].replace(\"Infinity\", 0, inplace=True)\n",
    "    df[\" Bwd Avg Bytes/Bulk\"].replace(\",,\", 0, inplace=True)\n",
    "    df[\" Bwd Avg Bytes/Bulk\"].replace(np.nan, 0, inplace=True)\n",
    "    df[\" Fwd Avg Bulk Rate\"].replace(\"Infinity\", 0, inplace=True)\n",
    "    df[\" Fwd Avg Bulk Rate\"].replace(\",,\", 0, inplace=True)\n",
    "    df[\" Fwd Avg Bulk Rate\"].replace(np.nan, 0, inplace=True)\n",
    "    df[\" Fwd Avg Packets/Bulk\"].replace(\"Infinity\", 0, inplace=True)\n",
    "    df[\" Fwd Avg Packets/Bulk\"].replace(\",,\", 0, inplace=True)\n",
    "    df[\" Fwd Avg Packets/Bulk\"].replace(np.nan, 0, inplace=True)\n",
    "    df[\"Fwd Avg Bytes/Bulk\"].replace(\"Infinity\", 0, inplace=True)\n",
    "    df[\"Fwd Avg Bytes/Bulk\"].replace(\",,\", 0, inplace=True)\n",
    "    df[\"Fwd Avg Bytes/Bulk\"].replace(np.nan, 0, inplace=True)\n",
    "    df[\" CWE Flag Count\"].replace(\"Infinity\", 0, inplace=True)\n",
    "    df[\" CWE Flag Count\"].replace(\",,\", 0, inplace=True)\n",
    "    df[\" CWE Flag Count\"].replace(np.nan, 0, inplace=True)\n",
    "    df[\" Bwd URG Flags\"].replace(\"Infinity\", 0, inplace=True)\n",
    "    df[\" Bwd URG Flags\"].replace(\",,\", 0, inplace=True)\n",
    "    df[\" Bwd URG Flags\"].replace(np.nan, 0, inplace=True)\n",
    "    df[\" Bwd PSH Flags\"].replace(\"Infinity\", 0, inplace=True)\n",
    "    df[\" Bwd PSH Flags\"].replace(\",,\", 0, inplace=True)\n",
    "    df[\" Bwd PSH Flags\"].replace(np.nan, 0, inplace=True)\n",
    "    df[\" Fwd URG Flags\"].replace(\"Infinity\", 0, inplace=True)\n",
    "    df[\" Fwd URG Flags\"].replace(\",,\", 0, inplace=True)\n",
    "    df[\" Fwd URG Flags\"].replace(np.nan, 0, inplace=True)\n",
    "\n",
    "    df[\"Flow Bytes/s\"]=df[\"Flow Bytes/s\"].astype(\"float64\")\n",
    "    df[' Flow Packets/s']=df[\" Flow Packets/s\"].astype(\"float64\")\n",
    "    df['Bwd Avg Bulk Rate']=df[\"Bwd Avg Bulk Rate\"].astype(\"float64\")\n",
    "    df[' Bwd Avg Packets/Bulk']=df[\" Bwd Avg Packets/Bulk\"].astype(\"float64\")\n",
    "    df[' Bwd Avg Bytes/Bulk']=df[\" Bwd Avg Bytes/Bulk\"].astype(\"float64\")\n",
    "    df[' Fwd Avg Bulk Rate']=df[\" Fwd Avg Bulk Rate\"].astype(\"float64\")\n",
    "    df[' Fwd Avg Packets/Bulk']=df[\" Fwd Avg Packets/Bulk\"].astype(\"float64\")\n",
    "    df['Fwd Avg Bytes/Bulk']=df[\"Fwd Avg Bytes/Bulk\"].astype(\"float64\")\n",
    "    df[' CWE Flag Count']=df[\" CWE Flag Count\"].astype(\"float64\")\n",
    "    df[' Bwd URG Flags']=df[\" Bwd URG Flags\"].astype(\"float64\")\n",
    "    df[' Bwd PSH Flags']=df[\" Bwd PSH Flags\"].astype(\"float64\")\n",
    "    df[' Fwd URG Flags']=df[\" Fwd URG Flags\"].astype(\"float64\")\n",
    "    pd.set_option('display.max_rows', df.shape[0])\n",
    "    df.replace('Infinity',0.0, inplace=True)\n",
    "    df.replace('NaN',0.0, inplace=True)\n",
    "\n",
    "\n",
    "    # old_memory_usage = df.memory_usage().sum()\n",
    "    #change the variable types for low memory usage\n",
    "    #int64 to int32,,, float64 to float32\n",
    "    integer = []\n",
    "    f = []\n",
    "    for i in df.columns[:-1]:\n",
    "        if df[i].dtype == \"int64\": integer.append(i)\n",
    "        else : f.append(i)\n",
    "\n",
    "    df[integer] = df[integer].astype(\"int32\")\n",
    "    df[f] = df[f].astype(\"float32\")\n",
    "\n",
    "    # df = df.replace((np.inf, -np.inf, np.nan), 0).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    # print(\"============================\")\n",
    "    # print(df.info())\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    168051\n",
      "1      2180\n",
      "Name:  Label, dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 170231 entries, 0 to 170365\n",
      "Data columns (total 15 columns):\n",
      " #   Column                        Non-Null Count   Dtype  \n",
      "---  ------                        --------------   -----  \n",
      " 0    Fwd Packet Length Max        170231 non-null  int32  \n",
      " 1    Flow IAT Std                 170231 non-null  float32\n",
      " 2    Fwd Packet Length Std        170231 non-null  float32\n",
      " 3   Fwd IAT Total                 170231 non-null  int32  \n",
      " 4    Flow Duration                170231 non-null  int32  \n",
      " 5    Fwd Packet Length Mean       170231 non-null  float32\n",
      " 6    Total Length of Bwd Packets  170231 non-null  int32  \n",
      " 7   Total Length of Fwd Packets   170231 non-null  int32  \n",
      " 8    Flow IAT Mean                170231 non-null  float32\n",
      " 9    Bwd Packet Length Mean       170231 non-null  float32\n",
      " 10   Flow IAT Max                 170231 non-null  int32  \n",
      " 11   Bwd Packet Length Std        170231 non-null  float32\n",
      " 12   Total Fwd Packets            170231 non-null  int32  \n",
      " 13   Total Backward Packets       170231 non-null  int32  \n",
      " 14   Label                        170231 non-null  int64  \n",
      "dtypes: float32(6), int32(8), int64(1)\n",
      "memory usage: 11.7 MB\n",
      "None\n",
      "0.9870764034218159\n",
      "[[26885    10]\n",
      " [  342     0]]\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.knn import KNN\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.hbos import HBOS\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.cblof import CBLOF\n",
    "from pyod.models.loci import LOCI\n",
    "from pyod.models.sos import SOS\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df6 = pd.read_csv(\"./data/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\")\n",
    "df6[\" Label\"] = df6[\" Label\"].replace([\"Web Attack � Brute Force\",\"Web Attack � XSS\",\"Web Attack � Sql Injection\"],\"Web Attack\")\n",
    "# print(df6[\" Label\"].unique())\n",
    "df6[' Label'].value_counts()\n",
    "\n",
    "df6[' Label'] = df6[' Label'].replace(['BENIGN'], 0)\n",
    "df6[' Label'] = df6[' Label'].replace(['Web Attack'], 1)\n",
    "\n",
    "df = preprosess(df6)\n",
    "\n",
    "features=[\" Fwd Packet Length Max\",\" Flow IAT Std\",\" Fwd Packet Length Std\" ,\"Fwd IAT Total\",' Flow Duration', \" Fwd Packet Length Mean\", \" Total Length of Bwd Packets\", \"Total Length of Fwd Packets\", \" Flow IAT Mean\", \" Bwd Packet Length Mean\",  \" Flow IAT Max\", \" Bwd Packet Length Std\", ' Total Fwd Packets', ' Total Backward Packets',' Label']\n",
    "\n",
    "# print(df6.columns)\n",
    "df = df[features].copy()\n",
    "\n",
    "# print(df3[\" Label\"].unique())\n",
    "print(df[' Label'].value_counts())\n",
    "print(df.info())\n",
    "\n",
    "X = df.drop([' Label'], axis=1)\n",
    "y = df[' Label']\n",
    "# X = df[df.columns[0:-1]]\n",
    "# y = df[df.columns[-1]]\n",
    "\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.2, random_state=10)\n",
    "\n",
    "# X_train = X_train.replace((np.inf, -np.inf, np.nan), 0).reset_index(drop=True)\n",
    "\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "# rus = RandomUnderSampler(sampling_strategy=1) # Numerical value\n",
    "# # rus = RandomUnderSampler(sampling_strategy=\"not minority\") # String\n",
    "# X_res, y_res = rus.fit_resample(X, y)\n",
    "\n",
    "# ax = y_res.value_counts().plot.pie(autopct='%.2f')\n",
    "# _ = ax.set_title(\"Under-sampling\")\n",
    "\n",
    "# # X_res.value_counts()\n",
    "# print(y_res.value_counts())\n",
    "\n",
    "X_train, X_test, y_train, y_test=train_test_split(X_train, y_train,test_size=0.2, random_state=10)\n",
    "\n",
    "\"\"\"\n",
    "# train kNN detector\n",
    "clf_name = 'KNN'\n",
    "# clf = KNN()\n",
    "clf = KNN(contamination=0.01)\n",
    "clf.fit(X_train)\n",
    "\n",
    "# On Training Data:\n",
    "# KNN ROC:0.1927, precision @ rank n:0.2119\n",
    "\n",
    "# On Test Data:\n",
    "# KNN ROC:0.2139, precision @ rank n:0.181\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "clf_name = 'LOF'\n",
    "# clf = LOF(contamination=0.000001)\n",
    "clf = LOF(contamination=0.1)\n",
    "\n",
    "clf.fit(X_train)\n",
    "\n",
    "# On Training Data:\n",
    "# LOF ROC:0.355, precision @ rank n:0.4153\n",
    "# On Test Data:\n",
    "# LOF ROC:0.3764, precision @ rank n:0.4\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "clf_name = 'IForest'\n",
    "clf = IForest(contamination=0.001)\n",
    "clf.fit(X_train)\n",
    "\n",
    "# On Training Data:\n",
    "# IForest ROC:0.0989, precision @ rank n:0.1648\n",
    "# On Test Data:\n",
    "# IForest ROC:0.081, precision @ rank n:0.1238\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "clf_name = 'HBOS'\n",
    "clf = HBOS(contamination=0.1)\n",
    "clf.fit(X_train)\n",
    "\n",
    "# HBOS ROC:0.4172, precision @ rank n:0.0036\n",
    "# On Test Data:\n",
    "# HBOS ROC:0.4117, precision @ rank n:0.0\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# # train LOCI detector\n",
    "# clf_name = 'LOCI'\n",
    "# clf = LOCI()\n",
    "# clf.fit(X_train)\n",
    "\n",
    "# print(X_train.shape)\n",
    "# # train SOS detector\n",
    "# clf_name = 'SOS'\n",
    "# clf = SOS()\n",
    "# clf.fit(X_train)\n",
    "\n",
    "\n",
    "\n",
    "# clf_name = 'CBLOF'\n",
    "# clf = CBLOF(contamination=0.1)\n",
    "# clf.fit(X_train)\n",
    "\n",
    "# # On Training Data:\n",
    "# # CBLOF ROC:0.8898, precision @ rank n:0.0\n",
    "\n",
    "# # On Test Data:\n",
    "# # CBLOF ROC:0.8938, precision @ rank n:0.0\n",
    "\n",
    "# # train CBLOF detector\n",
    "# clf_name = 'CBLOF'\n",
    "# clf = CBLOF(random_state=42)\n",
    "# clf.fit(X_train)\n",
    "# On Training Data:\n",
    "# CBLOF ROC:0.8898, precision @ rank n:0.0\n",
    "\n",
    "# On Test Data:\n",
    "# CBLOF ROC:0.8938, precision @ rank n:0.0\n",
    "\n",
    "\"\"\"\n",
    "from pyod.models.loda import LODA\n",
    "\n",
    "# train LOCI detector\n",
    "clf_name = 'LODA'\n",
    "clf = LODA()\n",
    "clf.fit(X_train)\n",
    "\n",
    "# On Training Data:\n",
    "# LODA ROC:0.1066, precision @ rank n:0.0\n",
    "\n",
    "# On Test Data:\n",
    "# LODA ROC:0.1014, precision @ rank n:0.0\n",
    "\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "clf_name = 'HBOS'\n",
    "clf = HBOS(contamination=0.09)\n",
    "clf.fit(X_train)\n",
    "\n",
    "# On Training Data:\n",
    "# HBOS ROC:0.4172, precision @ rank n:0.0036\n",
    "\n",
    "# On Test Data:\n",
    "# HBOS ROC:0.4117, precision @ rank n:0.0\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n",
    "y_train_scores = clf.decision_scores_  # raw outlier scores\n",
    "\n",
    "# get the prediction on the test data\n",
    "y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)\n",
    "y_test_scores = clf.decision_function(X_test)  # outlier scores\n",
    "\n",
    "# print(\"\\n\", y_train)\n",
    "# print(\"\\n\", y_train_scores)\n",
    "\n",
    "# evaluate and print the results\n",
    "print(\"\\nOn Training Data:\")\n",
    "evaluate_print(clf_name, y_train, y_train_scores)\n",
    "print(\"\\nOn Test Data:\")\n",
    "evaluate_print(clf_name, y_test, y_test_scores)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "\n",
    "# Initialize and train classifier model\n",
    "clf = LogisticRegression(solver='lbfgs', max_iter=200).fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compare test set predictions with ground truth labels\n",
    "print(accuracy_score(y_pred, y_test))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model\tAcc\tPr\tRecall\tF1\tExecution\n",
      "KNN\t0.998\t0.981\t0.898\t0.999\t20.20 secs\n",
      "CART\t0.995\t0.744\t0.877\t0.995\t5.69 secs\n",
      "RF\t0.994\t0.742\t0.869\t0.995\t4.62 secs\n",
      "ABoost\t0.995\t0.751\t0.891\t0.995\t72.68 secs\n",
      "LR\t0.987\t0.000\t0.000\t0.981\t44.50 secs\n",
      "NB\t0.865\t0.080\t0.870\t0.930\t0.87 secs\n",
      "LDA\t0.987\t0.470\t0.045\t0.982\t4.03 secs\n",
      "QDA\t0.779\t0.055\t0.987\t0.847\t1.83 secs\n",
      "MLP\t0.986\t0.083\t0.016\t0.979\t276.78 secs\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "models = []\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "# models.append(('SVM', SVC(gamma='auto')))\n",
    "models.append(('CART', DecisionTreeClassifier(max_depth=5)))\n",
    "models.append(('RF', RandomForestClassifier(max_depth=5, n_estimators=5, max_features=3)))    \n",
    "models.append(('ABoost', AdaBoostClassifier()))\n",
    "models.append(('LR', LogisticRegression(solver='lbfgs', max_iter=200)))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('QDA', QuadraticDiscriminantAnalysis()))\n",
    "models.append(('MLP', MLPClassifier()))\n",
    "\n",
    "print('Model\\tAcc\\tPr\\tRecall\\tF1\\tExecution')\n",
    "      \n",
    "for name, model in models:\n",
    "    start_time = time.time()\n",
    "    kfold = model_selection.KFold(n_splits=5, random_state=24, shuffle=True)    \n",
    "\n",
    "    accuracy = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy').mean()\n",
    "    precision = cross_val_score(model, X_train, y_train, cv=kfold, scoring='precision').mean()\n",
    "    recall = cross_val_score(model, X_train, y_train, cv=kfold, scoring='recall').mean()\n",
    "    f1_score = cross_val_score(model, X, y, cv=kfold, scoring='f1_weighted').mean()\n",
    "    \n",
    "    delta = time.time() - start_time\n",
    "    print('{}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.2f} secs'.format(name, accuracy, precision, recall, f1_score, delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikitplot.metrics import plot_confusion_matrix # For plotting confusion matrices\n",
    "from sklearn.metrics import classification_report # Various metrics for model performance\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=5, weights='uniform',\n",
    "                                    algorithm='auto', leaf_size=30,\n",
    "                                    p=2, metric='minkowski',\n",
    "                                    metric_params=None, n_jobs=None)\n",
    "\n",
    "model.fit(X_train, y_train.ravel()) # 4)\n",
    "pred_y = model.predict(X_test) # 5)\n",
    "score = classification_report(y_test, pred_y) # 6)\n",
    "print('Classification report: \\n', score, '\\n')\n",
    "\n",
    "plot_confusion_matrix(y_test, pred_y, title='Confusion Matrix for {}'.format(\"KNeighborsClassifier\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier # For Neural Network classifier\n",
    "\n",
    "Neural_Net_model = MLPClassifier(hidden_layer_sizes=(10,), activation='relu',\n",
    "                                    solver='adam', alpha=0.01, batch_size='auto',\n",
    "                                    learning_rate='adaptive', learning_rate_init=0.1,\n",
    "                                    max_iter=2)\n",
    "    \n",
    "# Fitting the model is synonymous to training the model. Need to call .ravel() to get array in correct format.\n",
    "Neural_Net_model.fit(X_train, y_train.ravel())\n",
    "\n",
    "# Using the model to predict the label/ classes, based upon X_test data only. This is the model's answers.\n",
    "pred_y = Neural_Net_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 588.3 MB 15 kB/s  eta 0:00:016   |███▉                            | 71.3 MB 1.1 MB/s eta 0:08:05     |█████▍                          | 99.0 MB 505 kB/s eta 0:16:08     |█████▊                          | 104.9 MB 161 kB/s eta 0:49:46     |███████                         | 128.5 MB 603 kB/s eta 0:12:43     |██████████████▏                 | 260.9 MB 183 kB/s eta 0:29:42     |█████████████████████           | 387.6 MB 771 kB/s eta 0:04:21     |████████████████████████▎       | 447.1 MB 965 kB/s eta 0:02:27     |████████████████████████████▊   | 528.5 MB 1.0 MB/s eta 0:00:59\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.12,>=2.11.0\n",
      "  Using cached tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.51.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.8 MB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-23.1.21-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow) (45.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.14.0)\n",
      "Collecting keras<2.12,>=2.11.0\n",
      "  Using cached keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1; platform_machine != \"arm64\" or platform_system != \"Darwin\"\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.4 MB 292 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=1.0.0\n",
      "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.2.0-py3-none-any.whl (6.6 kB)\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 627 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: packaging in /home/ebryx/.local/lib/python3.8/site-packages (from tensorflow) (21.3)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-15.0.6.1-py2.py3-none-manylinux2010_x86_64.whl (21.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 21.5 MB 1.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<2.12,>=2.11\n",
      "  Using cached tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "Collecting typing-extensions>=3.6.6\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/ebryx/.local/lib/python3.8/site-packages (from tensorflow) (1.23.1)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.15.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (81 kB)\n",
      "\u001b[K     |████████████████████████████████| 81 kB 685 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting h5py>=2.9.0\n",
      "  Downloading h5py-3.8.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.7 MB 588 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow) (0.34.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ebryx/.local/lib/python3.8/site-packages (from packaging->tensorflow) (3.0.9)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.16.1-py2.py3-none-any.whl (177 kB)\n",
      "\u001b[K     |████████████████████████████████| 177 kB 676 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.22.0)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
      "\u001b[K     |████████████████████████████████| 233 kB 719 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in /home/ebryx/.local/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (4.11.4)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/ebryx/.local/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ebryx/.local/lib/python3.8/site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (3.8.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.1.0)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Installing collected packages: tensorflow-estimator, gast, grpcio, flatbuffers, keras, astunparse, tensorflow-io-gcs-filesystem, absl-py, termcolor, protobuf, opt-einsum, libclang, markdown, requests-oauthlib, pyasn1, pyasn1-modules, rsa, cachetools, google-auth, google-auth-oauthlib, tensorboard-data-server, werkzeug, tensorboard-plugin-wit, tensorboard, typing-extensions, wrapt, h5py, google-pasta, tensorflow\n",
      "Successfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.0 flatbuffers-23.1.21 gast-0.4.0 google-auth-2.16.1 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.51.3 h5py-3.8.0 keras-2.11.0 libclang-15.0.6.1 markdown-3.4.1 opt-einsum-3.3.0 protobuf-3.19.6 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.2.0 typing-extensions-4.5.0 werkzeug-2.2.3 wrapt-1.15.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/ebryx/Documents/pcap_data_collections/server/anomaly-detection-Web Attack.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ebryx/Documents/pcap_data_collections/server/anomaly-detection-Web%20Attack.ipynb#Y113sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ebryx/Documents/pcap_data_collections/server/anomaly-detection-Web%20Attack.ipynb#Y113sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ebryx/Documents/pcap_data_collections/server/anomaly-detection-Web%20Attack.ipynb#Y113sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ebryx/Documents/pcap_data_collections/server/anomaly-detection-Web%20Attack.ipynb#Y113sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ebryx/Documents/pcap_data_collections/server/anomaly-detection-Web%20Attack.ipynb#Y113sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m Dense, Dropout, Activation\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from itertools import cycle\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "from typing import List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.preprocessing\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, BatchNormalization, Convolution1D, MaxPooling1D, Reshape\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "batch_size = 32\n",
    "model = Sequential()\n",
    "model.add(Convolution1D(64, kernel_size=32, padding=\"same\",\n",
    "          activation=\"relu\", input_shape=(76, 1)))\n",
    "model.add(MaxPooling1D(pool_size=(5)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=False)))\n",
    "model.add(Reshape((128, 1), input_shape=(128, )))\n",
    "\n",
    "model.add(MaxPooling1D(pool_size=(5)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=False)))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary(line_length=100)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = pd.DataFrame(columns=[1, 2, 3, 4, 5, 'Mean'],\n",
    "                      index=['Accuracy', 'Precision', 'Recall', 'F1'])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Copy of Copy of K fold Separate Notebook A.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
